{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MILK machine learning python tool package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Motivation - one paragraph explaining the main objective of the library and the problem it is trying to solve.\n",
    "The Milk library is a machine learning toolkit [1]. The main features Milk are based on supervised and unsupervised classification algorithms. It complements other packages by emphasis on speed and low memory usage. The milk package implements the performance sensitive code in C++ which gives it an upper hand in speed since C++ is a compiled language and python being interpreted language as we have learned in the class 1 lecture. This is behind Python-based interfaces for convenience. The major supervised classifiers covered by the milk package are Support Vector Machines, k-Nearest Neighbor, random forests, and decision trees. Milk supports k-means clustering (the ability to cluster millions of data points efficiently) and affinity propagation for unsupervised learning. Another main feature is feature selection for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Context - alternative solutions for solving the problem\n",
    "There are other machine learning libraries including scikit learn, theano, pylearn2, etc. to solve the classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Installation instructions, platform restriction and dependent libraries\n",
    "\n",
    "### Installation instructions\n",
    "- Step 1: Windows installation can be performed using the command prompt by simply entering \"pip install milk\".\n",
    "If the above step did not work; it might have been due to some of the system softwares not upgraded like c++ or visual basic etc.\n",
    "\n",
    "- Step 2: The alternative approach to install milk package is download a wheel file (Installing binary extensions [2]) from this website [3]. Choose the appropriate package file to download depending on the version of the python you are using on your computer. The commands 'import sys' and 'print (sys.version)' will let you know the version of python you are using. \n",
    "Once you have downloaded it onto your computer, open command prompt and go the directory that contains the downloaded wheel file and type 'pip install downloadedfilename'. This will solve the issue we have faced in step 1. The following video demonstrates the step 2 if something is not clear [4]. \n",
    "\n",
    "- To check the successful installation - type 'import milk' in python.\n",
    "\n",
    "### platform restriction \n",
    "#### Mac users\n",
    "- From the terminal run: \"pip install milk\"\n",
    "\n",
    "### dependent libraries\n",
    "- Milk package is dependent on numerical python (numpy) and is optimized for numpy arrays. It is also based on libsvm [5] for support vector machines classification algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import milk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?milk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Minimal working example\n",
    "\n",
    "There many supervised and unsupervised classifers we can choose using Milk package. Milk package supports changing various parameters to enable the user to control the classifier inputs (i.e., it is not like blackbox package).\n",
    "\n",
    "Here we demonstrated the minimal working example using the default supervised classifier in the Milk package which is the support vector machine. We can check the various options for the user control using the command '?milk.defaultclassifier'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data from ex2data csv file which has two features 'X1' and 'X2'. The class labels are in the column names 'class'\n",
    "df = pd.read_csv('ex2data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the observations with the two features 'X1' and 'X2'\n",
    "plt.scatter(df['X1'], df['X2'], c=df['class'], cmap=plt.cm.Paired)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initializing the classifier by defining the features and class labels\n",
    "features = np.array(df.iloc[:,:2])\n",
    "labels = np.array(df['class'])\n",
    "classifier = milk.supervised.defaultclassifier()#SVM classifier is the default supervised classifier\n",
    "\n",
    "#training the model\n",
    "model_defaultclassifier = classifier.train(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Classify new points\n",
    "# Testing the model with data point [-2,-3] which belongs to False class\n",
    "model_defaultclassifier.apply([-2,-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing the model with data point [3,2] which  belongs to True class\n",
    "model_defaultclassifier.apply([3,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 2-3 examples of typical use-cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a tree classifier using milk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building a tree using milk package\n",
    "#Define the tree\n",
    "tree=milk.supervised.tree_learner(min_split=1)\n",
    "#Developing a model using the defined tree with features and labels as the inputs\n",
    "model_treeClassifier = tree.train(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions using the tree model\n",
    "model_treeClassifier.apply([-2,-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_treeClassifier.apply([3,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier using C value and various kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building a SVM classifier using milk package. This time we specify C value and the kernel we want to use. \n",
    "# There are various kernels to explore using this package: \n",
    "# 'rbf_kernel',\n",
    "# 'polynomial_kernel',\n",
    "# 'precomputed_kernel',\n",
    "# 'dot_kernel',\n",
    "# 'svm_raw',\n",
    "# 'svm_binary',\n",
    "# 'svm_to_binary',\n",
    "# 'svm_sigmoidal_correction',\n",
    "# 'sigma_value_fisher',\n",
    "# 'fisher_tuned_rbf_svm',\n",
    "\n",
    "features = np.array(df.iloc[:,:2])\n",
    "labels = np.array(df['class'])\n",
    "classifier = milk.supervised.svm_simple(C=10,kernel=milk.supervised.svm.rbf_kernel(2.**-3))\n",
    "\n",
    "#training the model\n",
    "model_SVM_simple = classifier.train(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions using the SVM model\n",
    "model_SVM_simple.apply([-2,-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_SVM_simple.apply([3,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest neighbor supervised classifier using milk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import milk.supervised.knn\n",
    "\n",
    "X = np.array([[0,0,0],[1,1,1]])         \n",
    "Y = np.array([ 1, -1 ])\n",
    "\n",
    "#Defining the KNN model\n",
    "kNN = milk.supervised.knn.kNN(k=1)#'k=1' defines no. of neighbors to consider.\n",
    "#Training the model with vectors X and Y\n",
    "kNN = kNN.train(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing the KNN model\n",
    "kNN.apply(X[0]) == Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Testing the KNN model\n",
    "kNN.apply([0,0,1]) == Y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining feature selection using Milk package for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The dataset Auto has 8 feature vectors in total.\n",
    "\n",
    "# load data\n",
    "df1 = pd.read_csv('Auto.csv')\n",
    "\n",
    "#Converting horsepower feature from object to float\n",
    "from ipykernel import kernelapp as app\n",
    "df1['horsepower'] = df1['horsepower'].convert_objects(convert_numeric=True)\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the class labels suggesting low or high mileage for a given car with the feature vectors.\n",
    "# If the mileage of the car is less than the median of the mileage vector then its a low mileage and is represented with a '0' else '1' representing high mileage.\n",
    "\n",
    "mlgmedian=df1['mpg'].median()\n",
    "\n",
    "# #Creating the response variable based on median of mpg\n",
    "df1['labels'] = (df1['mpg'] > mlgmedian).astype(int)\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selecting the most important features using the milk package\n",
    "import milk.supervised.featureselection\n",
    "features = np.array(df1.iloc[:,:8])\n",
    "labels = np.array(df1['labels'])\n",
    "selected_features = milk.supervised.featureselection.sda(features,labels)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can observe from the above output of the selected_features; the vectors [7,0] in the df1 dataframe refer to the mpg and cyliners which came out to be the important predictors to determine the low or high mileage for a given car. \n",
    "\n",
    "The vector 7 which refers to mpg should obviously be the most important predictor since the response variable label is defined based on the median mpg value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning using Milk package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans unsupervised classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demonstrating Kmeans using Milk package\n",
    "df3 = pd.read_csv('kmeans_simple.csv')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize data\n",
    "df3.plot.scatter('x1','x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_ids,centroids=milk.unsupervised.kmeans(df3, 3, distance='euclidean', max_iter=1000)\n",
    "cluster_ids,centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bb=pd.DataFrame(centroids)\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting the centroids along with the datapoints. The 'x' marks on the plot represent the centroids obtained from the kmeans algorithm.\n",
    "bb.columns=['a','b']\n",
    "plt.scatter(bb['a'],bb['b'],marker='x',s=100)\n",
    "plt.scatter(df3['x1'],df3['x2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA using Milk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Defining random numbers. We considered 4 features in X\n",
    "np.random.seed(123)\n",
    "X = np.random.rand(10,4)\n",
    "X[:,1] += np.random.rand(10)**2*X[:,0] \n",
    "X[:,1] += np.random.rand(10)**2*X[:,0] \n",
    "X[:,2] += np.random.rand(10)**2*X[:,0] \n",
    "\n",
    "#Applying PCA using milk package\n",
    "Y,V = milk.unsupervised.pca(X, zscore=True)\n",
    "# Y is the transformed matrix with the same dimension of X\n",
    "# V contains the principal components\n",
    "Y,V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. List other interesting or useful features (additional examples are not required)\n",
    "\n",
    "- All the features of the MILK package are well explained in reference [6].\n",
    "\n",
    "- SVMs (Support Vector Machines) -  supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis [5 - 7]. \n",
    "\n",
    "- k-means - a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining [6, 8].\n",
    "\n",
    "- Random Forests - an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees [6, 9].\n",
    "\n",
    "- LASSO (least absolute shrinkage and selection operator) - a regression analysis method that performs variable selection and regularization to enhance prediction accuracy and interpretability of a statistical model [6, 10].\n",
    "\n",
    "- Self-organizing maps - a type of artificial neural network that is trained using unsupervised learning to produce a low-dimensional, discretized representation of the input space of the training samples, called a map. This is a method to reduce dimensionality [6, 11].\n",
    "\n",
    "- Stepwise Discriminant Analysis for feature selection -  a statistical analysis to predict a categorical dependent variable (called a grouping variable) by one or more continuous or binary independent variables (called predictor variables) [6, 12].\n",
    "\n",
    "- Non-negative matrix factorisation - a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements [6, 13].\n",
    "\n",
    "- Affinity propagation - a clustering algorithm based on the concept of \"message passing\" between data points [6, 14]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 7. Summary and personal assessment of the library\n",
    "\n",
    "The Milk package is a powerful machine learning toolkit. The speed at which it performs and the low memory usage make Milk an ideal machine learning tool for large datasets that would otherwise be timely to run using other kits. Additionally, the many features Milk encompasses makes it a versatile tool to run many different models for analysis. As seen from the examples in part 5, Milk's many features are used in different ways including supervised and unsupervised learning, cluster analysis, decision trees, and much more. \n",
    "\n",
    "As seen from the above examples in which we used the Milk package, we believe that it performed well for the different scenarios and datasets we used. Milk also demonstrated that it is very flexible with defining inputs into the model. Each of the different features we used performed the task we assigned to it the correct way, and it does it efficiently and effectively. We would recommend using the Milk package for any machine learning project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 8. References\n",
    "\n",
    "[1] https://pypi.python.org/pypi/milk/#downloads\n",
    "\n",
    "[2] https://docs.python.org/3/installing/\n",
    "\n",
    "[3] http://www.lfd.uci.edu/~gohlke/pythonlibs/\n",
    "\n",
    "[4] https://www.youtube.com/watch?v=jnpC_Ib_lbc\n",
    "\n",
    "[5] https://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
    "\n",
    "[6] http://pydoc.net/Python/milk/0.6.1/\n",
    "\n",
    "[7] https://en.wikipedia.org/wiki/Support_vector_machine\n",
    "\n",
    "[8] https://en.wikipedia.org/wiki/K-means_clustering\n",
    "\n",
    "[9] https://en.wikipedia.org/wiki/Random_forest\n",
    "\n",
    "[10] https://en.wikipedia.org/wiki/Lasso_(statistics)\n",
    "\n",
    "[11] https://en.wikipedia.org/wiki/Self-organizing_map\n",
    "\n",
    "[12] https://en.wikipedia.org/wiki/Discriminant_function_analysis\n",
    "\n",
    "[13] https://en.wikipedia.org/wiki/Non-negative_matrix_factorization\n",
    "\n",
    "[14] https://en.wikipedia.org/wiki/Affinity_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
